---
title: "Post-Mortem: The Day Redis Reached its Limits"
subtitle: "Why consolidating all your use cases into a single in-memory database is a ticking time bomb."
summary: "A weekend outage at Forest Admin caused by Redis memory exhaustion, highlighting the dangers of a single point of failure, data lifecycles, and scaling issues."
publishedAt: "2021-10-15"
readTimeMinutes: 5
tags:
  - backend
  - database
  - devops
---

_Note: This article was written post-mortem, after my departure from Forest Admin._

Storing complex and large objects in Redis for super-fast serving sounds like a great idea until it isn't. At Forest Admin, we learned the hard way that treating a single Redis instance as a silver bullet for everything can lead to catastrophic, cascading failures.

Here is the story of a weekend outage caused by an Out-Of-Memory (OOM) error, and the crucial architectural and lifecycle lessons we learned.

## The Context

To achieve sub-millisecond latency for some heavily accessed data, we built a use case that cached complex, large objects in Redis.

This feature was implemented 2 to 3 years before the incident. At the time, we had fewer clients, and the objects being stored were incredibly small anyway. In fact, most of the engineering team dealing with the incident (myself included) wasn't even at the company when this code was originally written.

Because Redis is incredibly fast and reliable, it had organically become our go-to solution for various other needs over time:

- **BullMQ**: Queueing and executing background jobs.
- **Authentication**: Managing user sessions.
- **Security**: Anti-brute force mechanisms and rate limiting.

All of these distinct processes were happily co-existing inside a **single Redis instance**.

## The Boiling Frog

As Forest Admin grew, so did our clients' use cases. What started as small, innocuous cached objects slowly expanded into massive data structures. Since the growth was gradual, it crept up on us silently.

Then, it happened on a weekend. A sudden spike in data volume pushed the caching mechanism over the edge. Before anyone noticed, our Redis instance hit its memory limit and exhausted its capacity.

The consequences were immediate and widespread. Because everything relied on that single instance, the issue wasn't isolated to just the caching layer:

- **Job Queues Stopped**: BullMQ couldn't write or read jobs, bringing background processing to a halt.
- **Authentication Broke**: Users couldn't log in or maintain their sessions.
- **Security Mechanisms Failed**: Our anti-brute force tracking went offline.

The entire platform was effectively suffering from a massive degradation, all stemming from one exhausted memory pool.

## The Root Causes

When we investigated the incident, several glaring failures in our infrastructure design and data lifecycle management stood out:

### 1. No Memory Alarms in Datadog

While we had monitoring in place, we hadn't set up specific alerts for Redis memory consumption. The instance silently ballooned until it crashed, giving us no preemptive warning to scale up or intervene before the limit was reached.

### 2. The Single Point of Failure (SPOF) Architecture

By coupling highly volatile, memory-intensive caching with mission-critical system infrastructure (like authentication and job queues) on the exact same instance, we created a massive single point of failure. A memory exhaustion issue in one domain entirely took down completely unrelated domains.

### 3. Inefficient Data Lifecycles

We initially thought we had data retention covered. We had a TTL (Time-To-Live) configured, but it simply wasn't aggressive enough. Furthermore, we realized that we weren't actively purging data belonging to deleted projectsâ€”leaving stale, bloated objects consuming precious RAM for absolutely no reason.

## The Fixes & Takeaways

We obviously split the architecture so that caching, queues, and sessions now live in isolated Redis instances. But we didn't stop there.

To fix the fundamental data issues, we:

- **Shortened the TTLs**: Making them much more aggressive to ensure memory is freed up faster.
- **Cleaned up Stale Data**: Implementing routines to immediately remove cached items belonging to deleted projects.
- **Compressed the Payloads**: A colleague took the time to study and implement a compression solution for the data _before_ it gets stored in Redis. Not only did this optimize our memory costs significantly, but it also resulted in faster HTTPS transport times due to the smaller payload sizes.

If you are using an in-memory database like Redis in production, this post-mortem serves as a reminder: always monitor your memory, isolate your functional domains, and never assume that a caching strategy that works for a small client base will automatically scale gracefully as your data complexity grows.
